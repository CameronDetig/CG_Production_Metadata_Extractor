services:
  # PostgreSQL database with pgvector extension
  postgres:
    image: pgvector/pgvector:pg16
    container_name: cg_postgres
    environment:
      POSTGRES_USER: cguser
      POSTGRES_PASSWORD: cgpass
      POSTGRES_DB: cg-metadata-db
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U cguser" ]
      interval: 5s
      timeout: 5s
      retries: 5

  metadata-extractor:
    build: .
    shm_size: 2gb
    container_name: cg_metadata_extractor
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      # Mount your data directory (read-only) - for local development only
      - ./cg-production-data:/app/cg-production-data:ro
      # Mount thumbnails directory
      - ./cg-production-data-thumbnails:/app/cg-production-data-thumbnails
      # Mount output directory for reports
      - ./output:/app/output
      # Mount model cache to avoid re-downloading on each build
      - model_cache:/root/.cache/huggingface
    environment:
      # Storage configuration
      - STORAGE_TYPE=local # Change to 's3' for AWS deployment

      # Local storage path (used when STORAGE_TYPE=local)
      - DATA_PATH=/app/cg-production-data
      - THUMBNAIL_PATH=/app/cg-production-data-thumbnails

      # S3 configuration (used when STORAGE_TYPE=s3)
      # - ASSET_BUCKET_NAME=cg-production-data
      # - THUMBNAIL_BUCKET_NAME=cg-production-data-thumbnails
      # - S3_PREFIX=
      # - AWS_REGION=us-east-1
      # Note: AWS credentials should be provided via IAM roles in AWS Batch

      # Database configuration - PostgreSQL with pgvector
      - DATABASE_URL=postgresql://cguser:cgpass@postgres:5432/cg-metadata-db

      # Logging
      - LOG_LEVEL=INFO

      # Resume support: skip files already in database (useful if scan crashes)
      # Set to 'false' to resume, 'true' (default) to re-process everything
      # - OVERRIDE_EXISTING=false

      # Parallel processing: number of workers for non-.blend files (default: 4)
      # .blend files are always processed sequentially to avoid memory issues
      # - SCANNER_WORKERS=4

volumes:
  model_cache:
