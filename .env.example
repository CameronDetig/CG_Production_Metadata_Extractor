# =============================================================================
# CG Production Data Assistant - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Storage Configuration
# -----------------------------------------------------------------------------

# Storage type: 'local' for filesystem, 's3' for AWS S3
STORAGE_TYPE=local

# Local storage paths (used when STORAGE_TYPE=local)
DATA_PATH=./cg-production-data
THUMBNAIL_PATH=./cg-production-data-thumbnails

# S3 configuration (used when STORAGE_TYPE=s3)
# Main asset bucket
ASSET_BUCKET_NAME=cg-production-data
# Thumbnail bucket (defaults to ASSET_BUCKET_NAME if not set)
THUMBNAIL_BUCKET_NAME=cg-production-data-thumbnails

# keep this empty if you wan to scan the entire bucket
S3_PREFIX=
AWS_REGION=us-east-1

# AWS credentials (NOT recommended for production - use IAM roles instead)
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------

# For Local PostgreSQL:
DATABASE_URL=postgresql://cguser:cgpass@localhost:5432/cg-metadata-db

# For AWS RDS PostgreSQL:
# DATABASE_URL=postgresql://username:password@rds-endpoint.region.rds.amazonaws.com:5432/cg-metadata-db

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Sequence Detection Settings
# Enable/disable automatic detection of file sequences (image sequences, cache files, etc.)
DETECT_SEQUENCES=true
# Minimum number of files required to be considered a sequence
MIN_SEQUENCE_LENGTH=5
# Number of parallel workers for processing non-.blend files
SCANNER_WORKERS=4
# If false, skip files already in database (useful for resuming crashed scans)
OVERRIDE_EXISTING=true

# -----------------------------------------------------------------------------
# AWS Batch Deployment Notes
# -----------------------------------------------------------------------------
# When deploying to AWS Batch:
# 1. Set STORAGE_TYPE=s3
# 2. Set S3_BUCKET_NAME and S3_PREFIX
# 3. Set DATABASE_URL to your RDS PostgreSQL connection string
# 4. DO NOT set AWS credentials - use IAM roles attached to Batch job
# 5. Ensure IAM role has:
#    - s3:GetObject, s3:ListBucket on your S3 bucket
#    - RDS connect permissions to your database
